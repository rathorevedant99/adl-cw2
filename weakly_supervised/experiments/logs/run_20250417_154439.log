2025-04-17 15:44:39,054 - root - INFO - Starting pipeline
2025-04-17 15:44:39,054 - root - INFO - Mode: train
2025-04-17 15:44:40,752 - root - INFO - Using device: cuda
2025-04-17 15:44:40,752 - root - INFO - Downloading dataset...
2025-04-17 15:44:40,752 - root - INFO - Downloading images...
2025-04-17 15:45:13,220 - root - INFO - Extracting images...
2025-04-17 15:45:24,273 - root - INFO - Downloading annotations...
2025-04-17 15:45:25,408 - root - INFO - Extracting annotations...
2025-04-17 15:45:31,617 - root - INFO - Organizing dataset files...
2025-04-17 15:45:32,184 - root - INFO - Dataset download and organization completed
2025-04-17 15:45:32,186 - root - INFO - Dataset download completed
2025-04-17 15:45:32,186 - root - INFO - Initializing dataset...
2025-04-17 15:45:32,186 - root - INFO - Initializing train dataset with weak_supervision=True
2025-04-17 15:45:32,186 - root - INFO - Creating train/val split files
2025-04-17 15:45:32,205 - root - INFO - Created classes.txt with 35 classes
2025-04-17 15:45:32,208 - root - INFO - Created split files: 5912 training samples, 1478 validation samples
2025-04-17 15:45:32,229 - root - INFO - Dataset initialized with 5912 images and 35 classes
2025-04-17 15:45:32,229 - root - INFO - Dataset initialized with 5912 samples
2025-04-17 15:45:32,229 - root - INFO - Initializing model...
2025-04-17 15:45:37,628 - root - INFO - Model initialized
2025-04-17 15:45:37,628 - root - INFO - Starting training...
2025-04-17 15:45:37,628 - root - INFO - Using device: cuda
2025-04-17 15:45:37,628 - root - INFO - Epoch 1/30
2025-04-17 15:45:47,555 - root - INFO - Batch 0/185, Loss: 5.3869
2025-04-17 15:45:51,460 - root - INFO - Batch 10/185, Loss: 7.5268
2025-04-17 15:45:55,363 - root - INFO - Batch 20/185, Loss: 1.8037
2025-04-17 15:45:59,265 - root - INFO - Batch 30/185, Loss: 0.9661
2025-04-17 15:46:03,172 - root - INFO - Batch 40/185, Loss: 1.0353
2025-04-17 15:46:07,078 - root - INFO - Batch 50/185, Loss: 1.1254
2025-04-17 15:46:10,983 - root - INFO - Batch 60/185, Loss: 0.6451
2025-04-17 15:46:14,887 - root - INFO - Batch 70/185, Loss: 0.8545
2025-04-17 15:46:18,793 - root - INFO - Batch 80/185, Loss: -0.4526
2025-04-17 15:46:22,694 - root - INFO - Batch 90/185, Loss: 0.6439
2025-04-17 15:46:26,596 - root - INFO - Batch 100/185, Loss: -0.0923
2025-04-17 15:46:30,504 - root - INFO - Batch 110/185, Loss: 0.4611
2025-04-17 15:46:34,410 - root - INFO - Batch 120/185, Loss: 0.5861
2025-04-17 15:46:38,316 - root - INFO - Batch 130/185, Loss: -0.3009
2025-04-17 15:46:42,216 - root - INFO - Batch 140/185, Loss: 0.9765
2025-04-17 15:46:46,124 - root - INFO - Batch 150/185, Loss: 0.5449
2025-04-17 15:46:50,032 - root - INFO - Batch 160/185, Loss: 0.0588
2025-04-17 15:46:53,940 - root - INFO - Batch 170/185, Loss: 0.0293
2025-04-17 15:46:57,846 - root - INFO - Batch 180/185, Loss: 0.7141
2025-04-17 15:46:59,714 - root - INFO - Epoch 1 completed. Average loss: 0.9636
2025-04-17 15:46:59,714 - root - INFO - Epoch 2/30
2025-04-17 15:47:09,243 - root - INFO - Batch 0/185, Loss: -0.0768
2025-04-17 15:47:13,143 - root - INFO - Batch 10/185, Loss: 0.9462
2025-04-17 15:47:17,045 - root - INFO - Batch 20/185, Loss: -0.1750
2025-04-17 15:47:20,948 - root - INFO - Batch 30/185, Loss: 0.6873
2025-04-17 15:47:24,850 - root - INFO - Batch 40/185, Loss: 0.0140
2025-04-17 15:47:28,756 - root - INFO - Batch 50/185, Loss: 0.2838
2025-04-17 15:47:32,667 - root - INFO - Batch 60/185, Loss: -0.0061
2025-04-17 15:47:36,573 - root - INFO - Batch 70/185, Loss: 1.0826
2025-04-17 15:47:40,483 - root - INFO - Batch 80/185, Loss: -0.2077
2025-04-17 15:47:44,393 - root - INFO - Batch 90/185, Loss: 0.0837
2025-04-17 15:47:48,305 - root - INFO - Batch 100/185, Loss: 0.8355
2025-04-17 15:47:52,214 - root - INFO - Batch 110/185, Loss: 0.0819
2025-04-17 15:47:56,126 - root - INFO - Batch 120/185, Loss: 0.1331
2025-04-17 15:48:00,037 - root - INFO - Batch 130/185, Loss: 0.2888
2025-04-17 15:48:03,946 - root - INFO - Batch 140/185, Loss: 0.3476
2025-04-17 15:48:07,868 - root - INFO - Batch 150/185, Loss: 0.1050
2025-04-17 15:48:11,779 - root - INFO - Batch 160/185, Loss: 0.1165
2025-04-17 15:48:15,699 - root - INFO - Batch 170/185, Loss: 0.6308
2025-04-17 15:48:19,614 - root - INFO - Batch 180/185, Loss: 0.0819
2025-04-17 15:48:21,558 - root - INFO - Epoch 2 completed. Average loss: 0.2828
2025-04-17 15:48:21,559 - root - INFO - Epoch 3/30
2025-04-17 15:48:31,456 - root - INFO - Batch 0/185, Loss: 0.6683
2025-04-17 15:48:35,367 - root - INFO - Batch 10/185, Loss: -0.1668
2025-04-17 15:48:39,284 - root - INFO - Batch 20/185, Loss: 0.3742
2025-04-17 15:48:43,200 - root - INFO - Batch 30/185, Loss: 0.1386
2025-04-17 15:48:47,128 - root - INFO - Batch 40/185, Loss: 0.9605
2025-04-17 15:48:51,042 - root - INFO - Batch 50/185, Loss: 0.7546
2025-04-17 15:48:54,962 - root - INFO - Batch 60/185, Loss: 0.1972
2025-04-17 15:48:58,875 - root - INFO - Batch 70/185, Loss: 0.0012
2025-04-17 15:49:02,797 - root - INFO - Batch 80/185, Loss: 0.5411
2025-04-17 15:49:06,722 - root - INFO - Batch 90/185, Loss: -0.3303
2025-04-17 15:49:10,665 - root - INFO - Batch 100/185, Loss: 0.7571
2025-04-17 15:49:14,597 - root - INFO - Batch 110/185, Loss: 0.0103
2025-04-17 15:49:18,538 - root - INFO - Batch 120/185, Loss: -0.1383
2025-04-17 15:49:22,460 - root - INFO - Batch 130/185, Loss: 0.0304
2025-04-17 15:49:26,380 - root - INFO - Batch 140/185, Loss: -0.0985
2025-04-17 15:49:30,288 - root - INFO - Batch 150/185, Loss: 0.2929
2025-04-17 15:49:34,196 - root - INFO - Batch 160/185, Loss: 0.2978
2025-04-17 15:49:38,104 - root - INFO - Batch 170/185, Loss: 0.7358
2025-04-17 15:49:42,013 - root - INFO - Batch 180/185, Loss: 0.1229
2025-04-17 15:49:43,863 - root - INFO - Epoch 3 completed. Average loss: 0.2783
2025-04-17 15:49:43,863 - root - INFO - Epoch 4/30
2025-04-17 15:49:53,625 - root - INFO - Batch 0/185, Loss: 0.3655
2025-04-17 15:49:57,541 - root - INFO - Batch 10/185, Loss: 0.4898
2025-04-17 15:50:01,450 - root - INFO - Batch 20/185, Loss: 0.1039
2025-04-17 15:50:05,357 - root - INFO - Batch 30/185, Loss: 0.2633
2025-04-17 15:50:09,270 - root - INFO - Batch 40/185, Loss: 0.0669
2025-04-17 15:50:13,179 - root - INFO - Batch 50/185, Loss: 0.0190
2025-04-17 15:50:17,088 - root - INFO - Batch 60/185, Loss: 0.2632
2025-04-17 15:50:20,996 - root - INFO - Batch 70/185, Loss: -0.0165
2025-04-17 15:50:24,905 - root - INFO - Batch 80/185, Loss: 0.3059
2025-04-17 15:50:28,813 - root - INFO - Batch 90/185, Loss: 0.1154
2025-04-17 15:50:32,721 - root - INFO - Batch 100/185, Loss: 0.1015
2025-04-17 15:50:36,628 - root - INFO - Batch 110/185, Loss: 0.4176
2025-04-17 15:50:40,538 - root - INFO - Batch 120/185, Loss: -0.0432
2025-04-17 15:50:44,443 - root - INFO - Batch 130/185, Loss: 0.4921
2025-04-17 15:50:48,356 - root - INFO - Batch 140/185, Loss: 0.0245
2025-04-17 15:50:52,267 - root - INFO - Batch 150/185, Loss: 0.2036
2025-04-17 15:50:56,173 - root - INFO - Batch 160/185, Loss: 0.5428
2025-04-17 15:51:00,082 - root - INFO - Batch 170/185, Loss: -0.2139
2025-04-17 15:51:03,993 - root - INFO - Batch 180/185, Loss: 0.0821
2025-04-17 15:51:05,880 - root - INFO - Epoch 4 completed. Average loss: 0.2782
2025-04-17 15:51:05,881 - root - INFO - Epoch 5/30
2025-04-17 15:51:15,919 - root - INFO - Batch 0/185, Loss: -0.2400
2025-04-17 15:51:19,833 - root - INFO - Batch 10/185, Loss: 0.3895
2025-04-17 15:51:23,760 - root - INFO - Batch 20/185, Loss: 0.5640
2025-04-17 15:51:27,674 - root - INFO - Batch 30/185, Loss: 0.3358
2025-04-17 15:51:31,593 - root - INFO - Batch 40/185, Loss: 0.8476
2025-04-17 15:51:35,514 - root - INFO - Batch 50/185, Loss: -0.3653
2025-04-17 15:51:39,446 - root - INFO - Batch 60/185, Loss: 0.5412
2025-04-17 15:51:43,367 - root - INFO - Batch 70/185, Loss: -0.2745
